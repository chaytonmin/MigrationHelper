# Evaluation

This folder contains all the data used for evaluation. 

`pic/` contains data and script to draw plots in the paper.
`deprecated/` contains some prior files that are not used for now.

`test-lib-input.txt` is the evaluation input libraries we use. 
It contains 220 libraries from the ground truth migration rules provided by [1].

`recommend-output*.csv` contains a list of recommendation libraries generated by one version of our algorithm.

## How do we extend original ground truth rules from [1]

[1] provides a basic list of possible Java library migrations rules mined from a large set of open source repositories and verified by experts (raw data available in `test-ground-truth-2014-raw.csv`). However, they only provide library names but not their group IDs and artifact IDs. Therefore, we apply the following processing steps.

1. Manually map them back to possible group IDs and artifact IDs, resulting in `test-ground-truth-2014.csv`.
2. Filter out libraries that is rarely used in our test data (This is done when we evaluate, but the data still contains all libraries).
3. If one rule item maps to multiple group IDs and artifact IDs, we consider their cartesian product as all valid rules.
4. For the remaining rules, we use the following heuristics to extend it: if (A, B) is a valid rule and (B, C) is a valid rule, then (A, C) is also an valid rule. (Not sure if this is needed)

We also plan to systematically extend the rules from the ground truth rules using our algorithm and compare with [1] to see how many new rules we can discover. THe final result should be in `test-ground-truth.csv`, but we are still working on that.

## RQ1: How many possible ground truth rules exist in data?

We analyze the number of occurrences of each ground truth rule we collected, and exclude those that exist in the data for very few times. 
For the rest, we manually investigate each rule, and mark a rule as true if we can find a relevant commit that explicitly states it is doing a migration.
Only those that satisfies the above conditions are kept as evaluation set.

See `depseq.ipynb`.

## RQ2: For those that exist in data, what's the efficiency of each metric?

This step will be similar to the original RQ1.
See `metrics.ipynb`.

## RQ3: How good is our algorithm in discovering these rules?

This step will be similar to the original RQ2.
See `evaluation.ipynb`.

## RQ4: How many additional migration rules can we discover, compared with [1]?

To be done.

## Failure case analysis

## References

1. Teyton, CÃ©dric, et al. "A study of library migrations in java." Journal of Software: Evolution and Process 26.11 (2014): 1030-1052.
